<!DOCTYPE html>

<html class="no-js" lang="en">

<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title>Phase Detection</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="../static/css/base.css">
    <link rel="stylesheet" href="../static/css/vendor.css">
    <link rel="stylesheet" href="../static/css/main.css">

    <!-- script
    ================================================== -->
    <script src="../static/js/modernizr.js"></script>
    <script src="../static/js/pace.min.js"></script>

    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
    <link rel="icon" href="../index.html" type="image/x-icon">

</head>


<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <nav class="header-nav-wrap">
            <ul class="header-nav">
                <li><a href="../index.html#home" title="home">Home</a></li>
                <li><a href="../index.html#about" title="about">Curriculum Vitae</a></li>
                <li class="current"><a href="../index.html#works" title="works">Projects</a></li>
                <li><a href="../index.html#blog" title="blog">Blog</a></li>
            </ul>
        </nav>

        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->


    <article class="project">

        <!-- page header/blog hero
        ================================================== -->
        <div class="project-header">

            <div class="project-header__content">

                <article>

                    <h1 class="project-header__title">
                      <p>Multibeat Echocardiographic Phase Detection Using Deep Neural Networks</p>
                      <div class="project-authors">
                      <p><i>Elisabeth S Lane <sup>1</sup>, Neda Azarmehr <sup>2</sup>, Jevgeni Jevsikov <sup>1</sup>, James P Howard <sup>2</sup>, Matthew J Shun-shin <sup>2</sup>, Graham D Cole <sup>2</sup>, Darrel P Francis <sup>2</sup>, Massoud Zolgharni <sup>1,2</sup><i></p>
                      </div>
                      <div class="affiliation">
                      <p><sup>1</sup> School of Computing and Engineering, University of West London, London, United Kingdom</p>
                      <p><sup>2</sup> National Heart and Lung Institute, Imperial College, London, United Kingdom</p>
                    </div>
                    </h1>

                </article>

            </div>

        </div> <!-- end page-header -->

        <div class="project-content">

            <div class="project-content__main">
                <h1>During this project I developed an automated model capable of identifying multiple end-systolic and end-diastolic frames in echocardiographic videos of arbitrary length with performance indistinguishable from that of human experts, but with significantly shorter processing time.</h1>
                <br>
                <a href="#" onclick='window.open("../static/docs/phase_detection.pdf");return false;' download="Multibeat Echocardiographic Phase Detection Using Deep Neural Networks" class="btn btn--primary">
                     Download the paper
                </a>
                <div = class="project-img">
                  <img src="../static/images/phase_detection/echo.gif" alt="echocardiogram">
                </div>

                <h1>Dataset</h1>
                <p>3 datasets were used in this study: 1 for training & testing, the others for testing only. The patient datasets and models are publicly available via
                <a href="#" onclick='window.open("https://github.com/elisabethlane/EchoPhaseDetection");return false;'>GitHub</a>, thereby providing a benchmark for future studies and allowing for external validation of the approach</p>
                <p>Additionally, annotations (ground-truth) from several cardiologist experts were used, allowing for the examination of inter- and intra-observer variability<p>

                <p>A summary of the datasets is as follows:</p>

               <div class="table-responsive">
                 <table class="table table-hover table-dark align-middle">
                   <thead>
                     <tr>
                       <th scope="col"><h3>Name</h3></th>
                       <th scope="col"><h3>PACS-dataset</h3></th>
                       <th scope="col"><h3>MultiBeat-dataset</h3></th>
                       <th scope="col"><h3>EchoNet-dataset</h3></th>
                     </tr>
                   </thead>
                   <tbody>
                     <tr>
                       <th scope="row">Source</th>
                       <td>Made <b>public</b> for this study<p>NHS Trust PACS Archives, Imperial College Healthcare</td>
                       <td><b>Private</b><p>St Mary’s Hospital</td>
                       <td><b>Public</b><p>Stanford University Hospital<p>echonet.github.io/dynamic</td>
                     </tr>
                     <tr>
                       <th scope="row">Ultrasound machine</th>
                       <td>Philips Healthcare (iE33 xMATRIX)</td>
                       <td>GE Healthcare (Vivid.i) and Philips Healthcare (iE33 xMATRIX)</td>
                       <td>Siemens Healthineers (Acuson SC2000) and Philips Healthcare (iE33, Epiq 5G, Epiq 7C)</td>
                     </tr>
                     <tr>
                       <th scope="row">Number of videos/patients</th>
                       <td>1,000</td>
                       <td>40</td>
                       <td>10,030</td>
                     </tr>
                     <tr>
                       <th scope="row">Length of videos</th>
                       <td>1-3 heartbeats</td>
                       <td>≥ 10 heartbeats</td>
                       <td>1 heartbeat</td>
                     </tr>
                     <tr>
                       <th scope="row">Ground-truth</th>
                       <td>2 annotations by 2 experts</td>
                       <td>6 annotations by 5 experts (twice by one expert)</td>
                       <td>1 annotation</td>
                     </tr>
                     <tr>
                       <th scope="row">Original size (pixels)</th>
                       <td>(300-768)×(400-1024)</td>
                       <td>422×636</td>
                       <td>112×112</td>
                     </tr>
                     <tr>
                       <th scope="row">Frame rate (fps)</th>
                       <td>23-102</td>
                       <td>52-80</td>
                       <td>50</td>
                     </tr>
                     <tr>
                       <th scope="row">Format</th>
                       <td>DICOM</td>
                       <td>DICOM</td>
                       <td>AVI</td>
                     </tr>
                     <tr>
                       <th scope="row">Use</th>
                       <td>Training/Testing</td>
                       <td>Testing</td>
                       <td>Testing</td>
                     </tr>
                   </tbody>
                 </table>
               </div>

              <br>
              <a href="#contact" class="btn btn--primary">Request access to the PACS-dataset</a>

              <h1>Network Architecture</h1>
              <p>Considering the patient image sequences as visual time-series, I adopted Long-term Recurrent Convolutional Networks (CNN+LSTM) for analysing the echocardiographic videos.</p>
              <p>Such architectures are a class of models that is both spatially and temporally deep, specifically designed for sequence prediction problems (e.g. order of images) with spatial inputs (e.g. 2D structure or pixels in an image).</p>
              <p>The figure below provides an overview of the network architecture.</p>
              <div class="project-img-lg">
                <img class="img-fluid" src="../static/images/phase_detection/echo_architecture.png" alt="echocardiogram">
              </div>
              <br>
              <p>The model comprises:</p>
              <p><b>(i) CNN unit:</b> for the encoding of spatial information for each frame of an echocardiographic video input</p>
              <p> <b>(ii) LSTM units:</b> for the decoding of complex temporal information</p>
              <p><b>(iii) a regression unit:</b> for the prediction of the frames of interest.</p>
              <br>
              <p>The steps are as follows:</p>
              <p><b>Spatial feature extraction:</b> First, a CNN unit is used to extract a spatial feature vector from every cardiac frame in the image sequence. A series of state-of-the-art architectures were employed for the CNN unit. These included ResNet50, InceptionV3, DenseNet, and InceptionResNetV2.</p>
              <p><b>Temporal feature extraction:</b> The CNN unit above is only capable of handling a single image, transforming it from input pixels into an internal matrix or vector representation. LSTM units are therefore used to process the image features extracted from the entire image sequence by the CNN, i.e. interpreting the features across time steps. Stacks of LSTM units (1-layer to 4-layers) were explored, where the output of each LSTM unit not in the final layer is treated as input to a unit in the next.</p>
              <p><b>Regression unit:</b> Finally, the output of the LSTM unit is regressed to predict the location of ED and ES frames. The model returns a prediction for each frame in the cardiac sequence (timestep).</p>


              <h1>Implementation</h1>
              <p>The models were implemented using the TensorFlow 2.0 deep learning framework and trained using an NVIDIA GeForce ® GTX 1080 Ti GPU.</p>
              <p>Random, on the fly augmentation prevented overfitting, such as rotating between -10 and 10 degrees and spatial cropping between 0 and 10 pixels along each axis.</p>
              <p>Throughout the study, training was conducted over 70 epochs with a batch size of 2 for all models.</p>
              <p>The PACS-dataset was used to train the models, with a data split of 60%, 20% and 20% for training, validation and testing, respectively.</p>
              <p>During testing, a sliding window of 30 frames in width with a stride of one was applied, allowing up to 30 predictions of differing temporal importance to be calculated for each timestep. Toward the end of each video, should a segment be fewer than 30 frames in length, it was zero-padded with the added frames removed after completion. Experimentation proved a stack of 2 LSTM layers was the optimum configuration across all models.</p>
              <br>
              <a class="btn btn--primary" href="#" onclick='window.open("https://github.com/elisabethlane/EchoPhaseDetection");return false;'>View project code</a>

              <h1>Evaluation metrics</h1>
              <br>
              <img class="img-fluid" src="../static/images/phase_detection/aaFD.png" alt="aaFD">
              <br><br>
              <p>As the primary endpoint for frame detection, evaluation of trained network predictions measures the difference between each labelled target, either ED or ES, and the timestep prediction.<p>
                Average Absolute Frame Difference (aaFD) notation is applied (to the left), where <i>N</i> is the number of events within the test dataset.<p>
               The signed mean (μ) and standard deviation (σ) of the error (i.e. frame differences) were also calculated.</p>

              <h1>Results</h1>
              <p><b>PACS-dataset:</b></p>
              <p>The average time (mean±SD) taken by the operators to manually annotate ED/ES frames was 26±11 seconds, per event. The equivalent time for our automated models, executed on the GPU, was less than 1.5 seconds; significantly faster than the human-led process.</p>
              <p>Examples of two random patient videos for which the frame detection error is zero, as well as when there is a disagreement between the model’s predictions and expert annotations can be seen below. </p>

              <div class="project-img">
                <img class="img-fluid" src="../static/images/phase_detection/pacs-examples.png" alt="pacs examples">
              </div><br>

              <p>The table below details the error in ED and ES frame detection for all videos in the PACS-dataset. The results indicate the level of disagreement between Operator-1 annotations, considered as the ground-truth, compared with automated predictions and those made by Operator-2.</p>

              <div class="project-img-lg">
                <img class="img-fluid" src="../static/images/phase_detection/pacs-results.png" alt="pacs results">
              </div><br>


              <p>The table below provides a comparison between the performance of the model and previously reported deep learning results.</p>
              <p>The model outperforms almost all existing approaches, indicating smaller discrepancies with the ground-truth from which it has learnt. However, caution is necessary, as different studies have used different private patient datasets, presumably with various levels of image quality and experience of human experts for annotations. Therefore, a direct comparison between the reported accuracies may not be as informative as desired. However, the proposed model’s removal of all pre-processing steps and its capacity to identify multiple heartbeats in one long video is an indisputable advantage.</p>
              <p>For details about the comparable studies, please see the references in our published paper</p>

              <div class="project-img">
                <img class="img-fluid" src="../static/images/phase_detection/previously-reported-results.png" alt="previously reported results">
              </div>

              <h1>Multibeat-dataset:</h1>
              <p>The Multibeat dataset was annotated by 5 experts, one of whom annotated twice for the evaluation of intra-observer variability. We refer to Operator-1a as the first set of annotations, and Opoerator-1b as the second set of annotations by the same expert</p>
              <p>The table below details detection errors between Operator-1 and detections made by the model and other operators. The model disagrees with Operator-1, as do Operators 2-5. Indeed, Operator-1 disagreed with themselves on their second annotation attempt. The smallest error was the discrepancy between the two annotations on separate occasions by the same operator (i.e. intra-observer variability), with a mean difference -0.22±2.76 and 0.25±3.75 for ED and ES events, respectively.</p>
              <p>The range of mean difference between two different operators (i.e. inter-observer variability) was [-0.87, -5.51]±[2.29, 4.26] and [-0.97, -3.46]±[3.67, 4.68] for ED and ES events, respectively. The model discrepancy falls within the range of inter-observer variability. Clearly demonstrating the reliability of the model in frame detection, compared with the experienced human experts.</p>

              <div class="project-img">
                <img class="img-fluid" src="../static/images/phase_detection/multibeat-results.png" alt="multibeat results">
              </div>

              <h1>EchoNet-dataset:</h1>
              <p>
                Previously, the proposed model was compared against alternative reported approaches. However, each study used a different private dataset, making a direct comparison extremely difficult. Here, we applied our model to the publicly available EchoNet-dataset, allowing for future studies to be benchmarked against ours. Like the MultiBeat-dataset, no further training was carried out, and the dataset was used in its entirety for testing.
                From the total number of videos (10,000), 810 were excluded owing to one of the ED or ES events occurring in the penultimate or final frame in the video, hence being unsuitable.  EchoNet was made available for a challenge focused on segmentation of the left ventricular. Therefore, it was acceptable to have ED or ES events occurring in first or last frames. The retained 9,190 videos were fed into the model, when no resampling of the images was required as the dataset is provided with a resolution of 112×112 pixels; identical to the input size of our model.
                An aaFD of 2.30 and 3.49 frames was obtained for ES and ES events, respectively and the mean frame difference was 0.16±3.56 and 2.64±3.59 for ED and ES; well within the range of inter-observer variability already observed.
              </p>

              <p>If you have any questions about this work, please email Elisabeth.Lane@uwl.ac.uk</p>

                <!-- ======= Contact Section ======= -->
              <section id="contact" class="contact">
                <hr>
                <h1>Request Access to the project dataset</h1>
                <p>The PACS-dataset used for training and testing our multibeat phase detection model has been made public for the purpose of benchmarking against future studies.</p>
                <p><b>If you would like to request access to the PACS-dataset, please complete the form below.<b></p>
                <p>Please note that the EchoNet dataset used for testing only in this study is available at the following URL: https://echonet.github.io/dynamic/</p>
                <h1>Important information about the PACS-dataset:</h1>
                  <p>We took a large random sample of echocardiographic studies from different patients performed between 2010 and 2020 from Imperial College Healthcare NHS Trust’s echocardiogram database. Ethical approval was obtained from the Health Regulatory Agency for the anonymised export of large quantities of imaging data. It was not necessary to approach patients individually for consent of data originally acquired for clinical purposes.
                    The images were acquired during examinations performed by experienced echocardiographers, according to the standard protocols for using ultrasound equipment from GE and Philips manufacturers. Only studies with full patient demographic data, and without intravenous contrast administration, were included. Automated anonymisation was performed to remove the patient-identifiable information.
                    A CNN model, previously developed in our research group to detect different echocardiographic views, was then used to identify and separate the A4C views. A total of 1,000 videos from different patients of varying lengths, were randomly selected.
                    Two accredited and experienced cardiology experts manually selected ED and ES frames, each blinded to the judgment of the other. We developed a custom-made program closely replicating the interface of clinical echocardiography hardware. Operators visually inspected the cine loops by controlled animation using a trackball, or arrow keys. The operators were asked to pick ED and ES frames in the A4C view, as they would in preparation for a Biplane Simpson’s measurement in clinical practice. All image sequences were down sampled by cubic interpolation into a standardised size of 112×112 pixels.<p>

                  <p>The image below shows a snapshot of the labels accompanying the dataset which you will receive once your access request has been accepted.</p>
                  <p>Op1 refers to operator 1, annotations from whom our network was trained/tested upon.</p>
                  <p>Op2 refers to operator 2, the second expert to annotate the dataset. Operator 2 annotations were used for testing and evaluation of observer variability only.</p>
                  <p>AVIname refers to the name of the avi file</p>
                  <p>Number Of Frames is the total number of frames in the video</p>
                  <p>Training/Testing refers to whether the video was used for training or testing only</p>
                  <p>Beats for ease of use, we have limited the number of beats in each video to 4. If you wish to have the unlimited beats label file, please specify when you request access to the dataset</p>
                  <p>Where there is no annotation in a cell, it means the first frame of interest is ES or the annotator did not select a frame.</p>
                  <img class="img-fluid" src="../static/images/phase_detection/labels-example.png" alt="example labels" >

                  <h1>If you wish to request access, please complete the form below:</h1>
                  <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">
                  <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" />
                  </a><p>This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</p></a>
                  <p> Please consider citing if you make use of the data:</p>
                  <i>Lane, Elisabeth S., Neda Azarmehr, Jevgeni Jevsikov, James P. Howard, Matthew J. Shun-Shin, Graham D. Cole, Darrel P. Francis, and Massoud Zolgharni. "Multibeat echocardiographic phase detection using deep neural networks." Computers in Biology and Medicine 133 (2021): 104373.</i></p>

                  <br>

                  <div class="row project__form">

                      <form action="https://formspree.io/f/myylgpzr" method="POST" role="form">

                        <fieldset>
                          <div class="form-field">
                            <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" required>
                          </div>
                          <div class="form-field">
                              <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" required>
                          </div>
                          <div class="form-field">
                              <input type="text" class="form-control" name="institution" id="subject" placeholder="Institution/Workplace" required>
                          </div>
                          <div class="form-field">
                              <textarea class="form-control" name="message" rows="5" placeholder="Please let us know why you would like access to this dataset and what you intend to use it for" required></textarea>
                          </div>
                          <div class="form-field">
                              <button type="submit">Submit</button>
                          </div>
                        </fieldset>
                    </form>

                  </div>
                </div>

              </section><!-- End Contact Section -->

            </article>
          <!-- footer
          ================================================== -->
          <footer>
                  <div class="row">
                      <div class="col-full">

                          <ul class="footer-social">
                              <li>
                                  <a href="#" onclick='window.open("https://www.linkedin.com/in/beth-lane/");return false;'><i class="im im-linkedin" aria-hidden="true"></i><span>LinkedIn</span></a>
                              </li>
                              <li>
                                  <a href="#" onclick='window.open("https://github.com/elisabethlane");return false;'><i class="im im-github" aria-hidden="true"></i><span>GitHub</span></a>
                              </li>
                              <li>
                                  <a href="#" onclick='window.open("https://scholar.google.com/citations?user=lIVUuT8AAAAJ");return false;'><i class="im im-pencil" aria-hidden="true"></i><span>Google Scholar</span></a>
                              </li>
                          </ul>

                      </div>
                  </div>

          </footer> <!-- end footer -->


          <div id="preloader">
              <div id="loader"></div>
          </div>


    <!-- Java Script
    ================================================== -->
    <script src="../static/js/jquery-3.2.1.min.js"></script>
    <script src="../static/static/js/plugins.js"></script>
    <script src="../static/js/main.js"></script>

</body>

</html>
